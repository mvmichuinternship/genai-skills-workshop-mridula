{
  "cells": [
    {
      "cell_type": "code",
      "id": "1iFuZ66ygo7bqDuk8K6n7Dko",
      "metadata": {
        "tags": [],
        "id": "1iFuZ66ygo7bqDuk8K6n7Dko"
      },
      "source": [
        "!pip install --upgrade google-cloud-bigquery google-cloud-aiplatform --quiet\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "import and initialise variables"
      ],
      "metadata": {
        "id": "Tte_dfefI_73"
      },
      "id": "Tte_dfefI_73"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery\n",
        "from google.cloud import aiplatform\n",
        "from vertexai.preview.generative_models import GenerativeModel\n",
        "\n",
        "PROJECT_ID = \"qwiklabs-gcp-02-598c62ac8986\"\n",
        "DATASET = \"challenge_2\"\n",
        "TABLE = \"challenge_2_embedded\"\n",
        "EMBEDDING_MODEL = \"Embeddings\"\n",
        "\n",
        "bq_client = bigquery.Client(project=PROJECT_ID)\n",
        "aiplatform.init(project=PROJECT_ID, location=\"us-central1\")"
      ],
      "metadata": {
        "id": "trbI_Rg9wGsW"
      },
      "id": "trbI_Rg9wGsW",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vector search functionality - load the results from the big query table"
      ],
      "metadata": {
        "id": "e3f_OUlIIuOF"
      },
      "id": "e3f_OUlIIuOF"
    },
    {
      "cell_type": "code",
      "source": [
        "def query_bigquery_vector_search(user_question: str, top_k: int = 3):\n",
        "\n",
        "    query = f\"\"\"\n",
        "SELECT\n",
        "  query.query,\n",
        "  result.base.content,\n",
        "  result.base.answer,\n",
        "  result.distance\n",
        "FROM VECTOR_SEARCH(\n",
        "  TABLE `{PROJECT_ID}.{DATASET}.{TABLE}`,\n",
        "  'ml_generate_embedding_result',\n",
        "  (\n",
        "    SELECT\n",
        "      ml_generate_embedding_result AS embedding,\n",
        "      '{user_question}' AS query\n",
        "    FROM ML.GENERATE_EMBEDDING(\n",
        "      MODEL `{PROJECT_ID}.{DATASET}.{EMBEDDING_MODEL}`,\n",
        "      (SELECT '{user_question}' AS content)\n",
        "    )\n",
        "  ),\n",
        "  top_k => {top_k},\n",
        "  options => '{{\"fraction_lists_to_search\": 1.0}}'\n",
        ") AS result\n",
        "\"\"\"\n",
        "    results = bq_client.query(query).to_dataframe()\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "UR_qxoWK1vps"
      },
      "id": "UR_qxoWK1vps",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieve the vector search results, set the context and prompt for the model and invoke the model to respond to the user's question."
      ],
      "metadata": {
        "id": "zAX4WWtcIWu8"
      },
      "id": "zAX4WWtcIWu8"
    },
    {
      "cell_type": "code",
      "source": [
        "def chatbot():\n",
        "    print(\"Chatbot: Hello! Ask me anything about your proprietary data.\")\n",
        "    while True:\n",
        "        user_input = input(\"\\nYou: \")\n",
        "        if user_input.lower() in ['exit', 'quit']:\n",
        "            print(\"Chatbot: Goodbye!\")\n",
        "            break\n",
        "\n",
        "        results = query_bigquery_vector_search(user_input)\n",
        "\n",
        "        if results.empty:\n",
        "            print(\"\\nChatbot: I couldn't find relevant information in my knowledge base.\")\n",
        "            continue\n",
        "\n",
        "        context = \"\"\n",
        "        for idx, row in results.iterrows():\n",
        "            context += f\"Q: {row['content']}\\nA: {row['answer']}\\n\\n\"\n",
        "\n",
        "        prompt = (\n",
        "            f\"Based on the following proprietary Q&A data, answer the user's question:\\n\\n\"\n",
        "            f\"{context}\\n\"\n",
        "            f\"User's question: {user_input}\\n\"\n",
        "            f\"Answer:\"\n",
        "        )\n",
        "\n",
        "        model = GenerativeModel('gemini-2.0-flash-001')\n",
        "        response = model.generate_content(prompt)\n",
        "        print(f\"\\nChatbot: {response.text.strip()}\")\n",
        "\n",
        "chatbot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgOZft-G15jq",
        "outputId": "f2c309f6-f94f-40e5-a4a8-becc885f08af"
      },
      "id": "vgOZft-G15jq",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot: Hello! Ask me anything about your proprietary data.\n",
            "\n",
            "You: when was aurora bay founded\n",
            "\n",
            "Chatbot: Aurora Bay was founded in 1901 by a group of fur traders who recognized the regionâ€™s strategic coastal location.\n",
            "\n",
            "You: exit\n",
            "Chatbot: Goodbye!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}